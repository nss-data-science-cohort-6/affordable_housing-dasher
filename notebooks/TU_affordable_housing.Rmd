---
title: "R TU Affordable Housing"
output: html_notebook
---

## Assessing the Impact of Affordable Housing Development

In the last decade, Davidson County has experienced tremendous population growth. With this population growth has come skyrocketing housing costs. From 2010 to 2019, both home values and rents have grown by more than 150%, while wages increased only slowly. High housing costs can squeeze household budgets, reducing the money available for other daily needs, including food, clothing, health care, utilities, and transportation, as well as money needed for education or for future savings.

One method of addressing rising housing costs is by building affordable housing developments. Despite the potential benefits, property owners who live near proposed housing developments often oppose such projects, citing fear that the developments will cause their property values to decline or will increase crime rates.

In this project, you'll be examining the impact of housing units built in Davidson under the the Low Income Housing Tax Credit (LIHTC) or which were funded by the Barnes Housing Trust Fund (<https://www.nashville.gov/departments/mayor/housing/barnes-fund>). Established in 1986, the LIHTC program has become an integral component of federal housing policy, funding 21 percent of all multifamily developments over the period 1987-2008. The Barnes Fund, created in 2013, is Metro Nashville's first housing trust fund and was created to leverage affordable housing developments throughout Davidson County.

You have been provided with several data sources. These data sources can be downloaded from here: <https://drive.google.com/drive/folders/1zWaLMIVQQuZXp4HHvtdeGpOZ5QYeWGPT?usp=share_link>

-   filtered_sales.csv: This dataset was obtained from the Nashville Planning Department's Parcel Viewer (<https://maps.nashville.gov/ParcelViewer/>). It contains sales of single family homes from 1995 to the present day. It has been filtered to remove sales that are likely not arms-length transactions and transactions for parcels which did not have a house on them at the time of sale. This was done by removing any transations for \$0, any transactions for which the adjacent appraisal values showed \$0 for improvents, and any for which the transaction amount was less than half of the adjacent appraisals. If you would like to work with the full dataset, all transactions are contained in full_sales.csv and the assessment values are in assessment.csv.

-   LIHTC.csv: Contains information on all Davidson County developments from the Department of Housing and Urban Development (HUD) National Low Income Housing Tax Credit Database. (For more information about the variables contained in this dataset, see the included data dictionary)

-   barnes.csv: Contains information on rental properties that are completed and have more than 10 units which were funded by the Barnes Fund.

-   property_details.csv: Contains detailed information on each property. Obtained from the Metro Nashville Assessor of Property's website (<https://www.padctn.org>). Includes year built, square footage, number of rooms, and location (lat/lng).

For this project, we'll be focusing on single family homes and look at affordable housing developments that were placed in service in 2000 or later.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(httr)
library(leaflet)
library(sf)
library(lubridate)
```

Before delving into the project itself, I'll start with some exploratory data analysis.

```{r message=FALSE, warning=FALSE}
filtered_sales <- read_csv('../data/filtered_sales.csv')
lihtc <- read_csv('../data/LIHTC_updated.csv')
barnes <- read_csv('../data/barnes.csv')
property_details <- read_csv('../data/property_details.csv')
```

Playing around with filtering and arranging (as well as referencing the website) I found out that: - NASHVILLE SENIOR CARE, LLC bought the most expensive property, for almost \$50 million. it's listed as "single family" which can be up to 4 units, owned or rented out, that doesn't seem to make much sense

    filtered_sales %>% 
      arrange(desc(amount))

-   there are 57 distinct transactions that went for the EXACT same price, \$22058426

With code below, changing the filter values: - there are 2147 unique sales values that appear at least 5 times

-   there are 1248 unique sales values that appear at least 10 times

-   there are 661 unique sales values that appear at least 30 times

-   there are 256 unique sales values that appear at least 100 times

```{=html}
<!-- -->
```
    filtered_sales %>% 
      distinct(apn, .keep_all= TRUE) %>% 
      group_by(amount) %>% 
      count() %>% 
      #filter(n >= 100)
      ggplot(aes(x=amount)) +
      geom_histogram(binwidth=10000, na.rm=TRUE) +
      xlim(0,1000000)

Interesting. I didn't quite expect that many transactions to be done at the exact same value down to the single \$.

First sale was made 1/1/1995, last one 10/27/2022. Most distinct sales happened in 1995, gradually going down, with an especially noticeable dip during the market crash.

    filtered_sales %>% 
      distinct(apn, .keep_all= TRUE) %>% 
      mutate(owneryear=format(ownerdate,'%Y')) %>% 
      group_by(owneryear) %>% 
      count() %>% 
      ggplot(aes(y=owneryear, weight=n)) +
      geom_bar()

    filtered_sales %>% 
      #distinct(apn, .keep_all= TRUE) %>% 
      mutate(owneryear=format(ownerdate,'%Y')) %>% 
      group_by(owneryear) %>% 
      summarize(max_price = max(amount), min_price = min(amount)) %>% 
      mutate(price_diff = max_price - min_price) %>% 
      arrange(desc(price_diff))

Biggest price difference (between min and max sale) was in 2018, no surprise there. HHowever, it's interesting to me that the fourth biggest was in 2006.

so 177 LIHTC projects, most of them placed in service in 1988 and 1989. There's 56 that were placed in service after (including) year 2000, not including the 2 placed in service unknown when and the 3 unconfirmed.

    lihtc %>% 
      filter(YR_PIS >= 2000) %>% 
      group_by(YR_PIS) %>% 
      count()

## Part 1: Statistical Analysis

For the first part of this project, you'll mimic the methodology of the working paper "Does Federally Subsidized Rental Housing Depress Neighborhood Property Values?" (<https://furmancenter.org/research/publication/does-federally-subsidized-rental-housing-depress-neighborhood-property>), building a statistical model to explore the effect on sales price of a home being within a half mile of an affordable housing development.

> why 2000 ft??? in VA paper, it's 1/16 of a mile, 330 feet, paragonable to a city block; up to one mile (5280 ft)

1.  Using the sf library, find the closest development to each home. Hint: You can convert a tibble to an sf object using the st_as_sf function (<https://r-spatial.github.io/sf/reference/st_as_sf.html>). See, for example, this stackoverflow post: <https://gis.stackexchange.com/questions/222978/lon-lat-to-simple-features-sfg-and-sfc-in-r>. Once converted, you can use the st_nearest_feature function (<https://r-spatial.github.io/sf/reference/st_nearest_feature.html>).

> first of all, have to filter duplicate sales by doing this, removed duplicate sales removed 7507 lines, coming to 311327 observations. Next, keep only the ones that have information in property_details (it appears all of them do). Additional note: by doing a full join and filter(is.na(amount)), found out that there are 33566 values that are present in property_details but we have no info on sales.

```{r}
property_sales_details <- filtered_sales %>% 
  distinct() %>% 
  inner_join(property_details, by='apn') %>% 
  mutate(
    longitude=str_extract(centroid, '-\\d*.\\d*'),
    latitude=str_remove(str_extract(centroid, ',\\d*.\\d*'),',')
  ) 

```

> Secondly, since we only want affordable housing since year 2000, filter out the dataset, going from 177 rows down to 57; but then we can add the barnes dataset to it, remove all not relevant columns for our analysis. Once that is done, there is a total of 66 rows

```{r}
barnes <- barnes %>% 
  mutate(HUD_ID=c('TNA081023A00400CO',
                  'TNA09211010400',
                  'TNA081023A90000CO',
                  'TNA08600035500',
                  'TNA10613000800',
                  'TNA05108017800',
                  'TNA09311022700',
                  'TNA06912006600',
                  'TNA10601016800'),
         .before = 'Barnes Year') %>% 
  select(HUD_ID, YR_PIS='Barnes Year', LATITUDE= lat, LONGITUDE=lng)


lihtc <- lihtc %>% 
  filter(YR_PIS >= 2000 & !YR_PIS %in% c(8888, 9999)) %>% 
  select(HUD_ID, YR_PIS, LATITUDE, LONGITUDE)

total_HUD <- bind_rows(lihtc,barnes)
```


```{r}
property_sales_details_sf <- property_sales_details %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant")

total_HUD_sf <- total_HUD %>% 
  st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326, agr = "constant")
```

```{r}
sales_and_HUD <- property_sales_details_sf %>% 
  mutate(HUD_ID = total_HUD$HUD_ID[st_nearest_feature(property_sales_details_sf ,total_HUD_sf)]) %>% 
  left_join(total_HUD, by='HUD_ID')
```

2.  Calculate the distance from each home its closest development.

```{r}
nearest_HUD <- total_HUD_sf[st_nearest_feature(property_sales_details_sf, total_HUD_sf), ]
sales_and_HUD$nearest_HUD_dist <- st_distance(sales_and_HUD, nearest_HUD, by_element = T)

```

3.  Filter the homes down to those that are within one mile of an affordable housing development.

4.  For each remaining home, calculate a new column called "group", which is defined according to the following rules. Hint: Use the case_when function to do this.

-   "pre" - for homes where the distance is less than half a mile and whose sale date was 2-5 years prior to the input year
-   "mid" - for homes where the distance is less than half a mile and whose sale date was 0-2 years prior to the input year
-   "post" - for homes where the distance is less than half a mile and whose sale date was after the input year
-   "outside" - for homes where the distance is more than half a mile and whose sale date was no more than 5 years prior to the input year
-   "other" - All other rows

> note: 1 mile = 1609.34 m - this filters it down to 112,085 values

5.  Filter out all rows whose group is "other".

> down to 72,494 rows

6.  Add an id column containing the id value for the development.

> already done that with the modifications I had to do earlier, the HUD_ID

7.  Create a column "Tpost" that, for homes in the "post" group gives the number of years that the sale occurred after the housing development was placed in service.

8.  Create a column named "age" which gives the age of the home at the time of sale.

```{r}
sales_and_HUD <- sales_and_HUD %>% 
  filter(as.numeric(nearest_HUD_dist) <= 1609.34) %>% 
  mutate(group=case_when(
    as.numeric(nearest_HUD_dist) < 804.67 &
    (YR_PIS-year(ownerdate)) %in% 2:5 ~ 'pre',
    as.numeric(nearest_HUD_dist) < 804.67 &
    (YR_PIS-year(ownerdate)) %in% 0:1 ~ 'mid',
    as.numeric(nearest_HUD_dist) < 804.67 &
    (YR_PIS-year(ownerdate)) < 0 ~ 'post',
    as.numeric(nearest_HUD_dist) >= 804.67 &
    (YR_PIS-year(ownerdate)) <= 5 ~ 'outside',
    TRUE ~ 'other'
  )) %>% 
  filter(group!='other') %>% 
  mutate(Tpost=if_else(group == 'post', year(ownerdate)-YR_PIS, as.numeric(NA), ),
         age=year(ownerdate)-year_built)
```

9.  Filter down to only sales that took place within the five years before or after the associated development was placed in service. Then build a linear model with target variable the sales amount using the following features:

-   square_footage
-   age of home at time of sale
-   group
-   year
-   tract
-   How can you interpret the coefficients of this model?

```{r}
sales_and_HUD
```

```{r}
sales_and_HUD_lm <- sales_and_HUD %>% 
  filter(abs(year(ownerdate)-YR_PIS) <=5) %>% 
  lm(amount ~ square_footage + age + group + year(ownerdate) + tract, data = .)

anova(sales_and_HUD_lm)
summary(sales_and_HUD_lm)
```

> The intercept in this case doesn't make much sense. The square footage and year (bought) both have a positive inpact on increasing the price, as well as the group outside of the housing unit.

10. Now, try a model with target being the log of the sale price.

-   square_footage
-   age of home at time of sale
-   group
-   year
-   tract
-   How can you interpret the coefficients of this model?

```{r}
sales_and_HUD_lmlog <- sales_and_HUD %>% 
  filter(abs(year(ownerdate)-YR_PIS) <=5) %>% 
  lm(log(amount) ~ square_footage + age + group + year(ownerdate) + tract, data = .)

anova(sales_and_HUD_lmlog)
summary(sales_and_HUD_lmlog)
```

11. Continue to explore the data to see if you can improve the models you have.
